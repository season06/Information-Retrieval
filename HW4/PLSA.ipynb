{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFile():\n",
    "    query = {}\n",
    "    doc = {}\n",
    "    root_path = './data'\n",
    "\n",
    "    with open(f'{root_path}/query_list.txt', 'r') as file:\n",
    "        query_list = file.read()\n",
    "        for file_name in query_list.split('\\n'):\n",
    "            try:\n",
    "                file_path = f'{root_path}/queries/{file_name}.txt'\n",
    "                with open(file_path, 'r') as f:\n",
    "                    query[file_name] = f.read().lower()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    with open(f'{root_path}/doc_list.txt', 'r') as file:\n",
    "        doc_list = file.read()\n",
    "        for file_name in doc_list.split('\\n'):\n",
    "            try:\n",
    "                file_path = f'{root_path}/docs/{file_name}.txt'\n",
    "                with open(file_path, 'r') as f:\n",
    "                    doc[file_name] = f.read().lower()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    print(len(query))\n",
    "    print(len(doc))\n",
    "    \n",
    "    return query, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './data/queries/.txt'\n",
      "[Errno 2] No such file or directory: './data/docs/.txt'\n",
      "100\n",
      "14955\n"
     ]
    }
   ],
   "source": [
    "query_dict, doc_dict = openFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tf(doc_dict):\n",
    "    tf_dict = {}   # tf[doc][word]\n",
    "    word_dict = {} # word: count\n",
    "    all_word_len = 0\n",
    "    \n",
    "    for doc_name, doc in doc_dict.items(): # 讀取 key value\n",
    "        tf_dict[doc_name] = {}\n",
    "        all_word_len += len(doc.split(' '))\n",
    "        \n",
    "        for word in doc.split(' '):        # 將 document 的內容拆成 token\n",
    "                \n",
    "            if tf_dict[doc_name].get(word, 0):  # 計算 tf\n",
    "                tf_dict[doc_name][word] += 1\n",
    "            else:\n",
    "                tf_dict[doc_name][word] = 1\n",
    "                \n",
    "            if word_dict.get(word, 0):  # 計算 tf\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "        \n",
    "    return tf_dict, word_dict, all_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7059938\n"
     ]
    }
   ],
   "source": [
    "tf_dict, word_dict, all_word_len = cal_tf(doc_dict)\n",
    "print(all_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10333\n"
     ]
    }
   ],
   "source": [
    "new_word_dict = {}\n",
    "word2id = {}\n",
    "i = 0\n",
    "\n",
    "query_word = []\n",
    "for _, value in query_dict.items():\n",
    "    query_word.append(value.split(' '))\n",
    "query_word = sum(query_word, [])\n",
    "\n",
    "# select word if word in query or tf > 30\n",
    "for word in list(word_dict.keys()):\n",
    "    if word in query_word or word_dict[word] > 40:\n",
    "        new_word_dict[word] = word_dict[word]\n",
    "\n",
    "        word2id[word] = i\n",
    "        i += 1\n",
    "\n",
    "print(len(new_word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate BG word\n",
    "BG_word = {}\n",
    "for word, count in new_word_dict.items():\n",
    "    BG_word[word] = count / all_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialParameter(doc_len, word_len, K):\n",
    "    T_w = np.random.random([K, word_len])\n",
    "    d_T = np.random.random([doc_len, K])\n",
    "    \n",
    "    for k in range(0, K):\n",
    "        normalization = sum(T_w[k, :])\n",
    "        for i in range(0, word_len):\n",
    "            T_w[k, i] /= normalization\n",
    "\n",
    "    for j in range(0, doc_len):\n",
    "        normalization = sum(d_T[j, :])\n",
    "        for k in range(0, K):\n",
    "            d_T[j, k] /= normalization\n",
    "            \n",
    "    return T_w, d_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 12202)\n",
      "(14955, 8)\n",
      "(14955, 12202, 8)\n"
     ]
    }
   ],
   "source": [
    "doc_list, word_list = list(doc_dict.keys()), list(word_dict.keys())\n",
    "doc_len, word_len = len(doc_dict), len(new_word_dict)\n",
    "\n",
    "# number of TOPIC\n",
    "K = 8\n",
    "\n",
    "# T_W[topic][word] : p(wi|Tk)\n",
    "# D_T[doc][topic] : p(Tk|dj)\n",
    "# e_step[doc][word][topic] : p(Tk|wi,dj)\n",
    "T_w, d_T = initialParameter(doc_len, word_len, K)\n",
    "print(T_w.shape)\n",
    "print(d_T.shape)\n",
    "\n",
    "e_step = np.zeros([doc_len,word_len,K])\n",
    "print(e_step.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EStep(doc_len, word_len, tf_dict, K, T_w, d_T, e_step):\n",
    "    print('EStep: ', time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "    \n",
    "    for i in range(0, doc_len):\n",
    "        for j in range(0, word_len):\n",
    "            \n",
    "            doc, word = doc_list[i], word_list[j]\n",
    "            if not tf_dict[doc].get(word, 0):\n",
    "                continue\n",
    "                \n",
    "            denominator = 0\n",
    "            \n",
    "            for k in range(0, K):\n",
    "                e_step[i][j][k] = T_w[k][j] * d_T[i][k]\n",
    "                denominator += e_step[i][j][k]\n",
    "                \n",
    "            if denominator == 0:\n",
    "                for k in range(0, K):\n",
    "                    e_step[i][j][k] = 0\n",
    "            else:\n",
    "                for k in range(0, K):\n",
    "                    e_step[i][j][k] /= denominator\n",
    "    \n",
    "    return e_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MStep(doc_len, word_len, tf_dict, K, T_w, d_T, e_step):\n",
    "    print('MStep: ', time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "    \n",
    "    ### update T_w : p(wi|Tk)\n",
    "    for k in range(0, K):\n",
    "        denominator = 0\n",
    "        \n",
    "        for j in range(0, word_len):\n",
    "            T_w[k][j] = 0\n",
    "            \n",
    "            for i in range(0, doc_len):\n",
    "                doc, word = doc_list[i], word_list[j]\n",
    "                if tf_dict[doc].get(word, 0):\n",
    "                    T_w[k][j] += tf_dict[doc][word] * e_step[i][j][k]\n",
    "            denominator += T_w[k][j]\n",
    "            \n",
    "        if denominator == 0:\n",
    "            for j in word_len:\n",
    "                T_w[k][j] = 1.0 / word_len\n",
    "        else:\n",
    "            for j in range(0, word_len):\n",
    "                T_w[k][j] /= denominator\n",
    "                \n",
    "    ### update d_T : p(Tk|dj)\n",
    "    for i in range(0, doc_len):\n",
    "        for k in range(0, K):\n",
    "            d_T[i][k] = 0\n",
    "            denominator = 0\n",
    "            \n",
    "            for j in range(0, word_len):\n",
    "                doc, word = doc_list[i], word_list[j]\n",
    "                if tf_dict[doc].get(word, 0):\n",
    "                    d_T[i][k] += tf_dict[doc][word] * e_step[i][j][k]\n",
    "                    denominator += tf_dict[doc][word]\n",
    "                \n",
    "            if denominator == 0:\n",
    "                d_T[i][k] = 1.0 / K\n",
    "            else:\n",
    "                d_T[i][k] /= denominator\n",
    "                \n",
    "    return T_w, d_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Likelihood(doc_len, word_len, tf_dict, K, T_w, d_T):\n",
    "    likelihood = 0\n",
    "    \n",
    "    for i in range(0, doc_len):\n",
    "        for j in range(0, word_len):\n",
    "            tmp = 0\n",
    "            \n",
    "            for k in range(0, K):\n",
    "                tmp += T_w[k][j] * d_T[i][k]\n",
    "                \n",
    "            if tmp > 0:\n",
    "                doc, word = doc_list[i], word_list[j]\n",
    "                if tf_dict[doc].get(word, 0):\n",
    "                    likelihood += tf_dict[doc][word] * math.log(tmp, 10)\n",
    "                \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_algorithm(doc_len, word_len, tf_dict, K, T_w, d_T, e_step):\n",
    "    Iteration = 20\n",
    "    threshold = 100.0\n",
    "    oldLoglikelihood = 1\n",
    "    newLoglikelihood = 1\n",
    "\n",
    "    for i in range(0, Iteration):\n",
    "        e_step = EStep(doc_len, word_len, tf_dict, K, T_w, d_T, e_step)\n",
    "        T_w, d_T = MStep(doc_len, word_len, tf_dict, K, T_w, d_T, e_step)\n",
    "        newLoglikelihood = Likelihood(doc_len, word_len, tf_dict, K, T_w, d_T)\n",
    "        \n",
    "        print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
    "        \n",
    "        if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
    "            break\n",
    "        oldLoglikelihood = newLoglikelihood\n",
    "    \n",
    "    return T_w, d_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLSA_model(query, doc_dict, tf_dict, BG_word, K, T_w, d_T):\n",
    "    alpha, beta = 0.8, 0.2\n",
    "    score_dict = {}\n",
    "    \n",
    "    for i in range(0, len(doc_list)):\n",
    "        doc_name = doc_list[i]\n",
    "        doc = doc_dict[doc_name]\n",
    "        doc_len = len(doc)\n",
    "        score = 0\n",
    "        \n",
    "        for word in query.split(' '):\n",
    "            tf = tf_dict[doc_name].get(word, 0)  # 將 word 轉成 score\n",
    "            \n",
    "            tmp = 0\n",
    "            id_word = word2id[word]\n",
    "            for k in range(0, K):\n",
    "                tmp += T_w[k][id_word] * d_T[i][k]\n",
    "            \n",
    "            first = alpha * (tf / doc_len)\n",
    "            second = beta * tmp\n",
    "            third = (1 - alpha - beta) * BG_word[word]\n",
    "            \n",
    "            score = first + second + third\n",
    "            \n",
    "        score_dict[doc_name] = score\n",
    "    \n",
    "    rank = sorted(score_dict.items(), key=lambda x: x[1], reverse = True) # 根據分數做排序\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EStep:  2020-11-24 01:38:47\n",
      "MStep:  2020-11-24 01:40:09\n",
      "[ 2020-11-24 02:13:32 ]  1  iteration   -21131129.973737363\n"
     ]
    }
   ],
   "source": [
    "T_w, d_T = EM_algorithm(doc_len, word_len, tf_dict, K, T_w, d_T, e_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "f = open('ans.txt', 'w')\n",
    "string = 'Query,RetrievedDocuments\\n'\n",
    "\n",
    "for _id, _query in query_dict.items():\n",
    "    rank = PLSA_model(_query, doc_dict, tf_dict, BG_word, K, T_w, d_T)\n",
    "r\n",
    "    string += _id + ','\n",
    "    for i, doc in enumerate(rank):\n",
    "        string += doc[0] + ' '\n",
    "        \n",
    "        if i == 999:\n",
    "            break\n",
    "    string += '\\n'\n",
    "    \n",
    "f.write(string)\n",
    "f.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
