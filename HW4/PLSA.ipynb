{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFile():\n",
    "    query = {}\n",
    "    doc = {}\n",
    "    root_path = './data'\n",
    "\n",
    "    with open(f'{root_path}/query_list.txt', 'r') as file:\n",
    "        query_list = file.read()\n",
    "        for file_name in query_list.split('\\n'):\n",
    "            try:\n",
    "                file_path = f'{root_path}/queries/{file_name}.txt'\n",
    "                with open(file_path, 'r') as f:\n",
    "                    query[file_name] = f.read().lower()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    with open(f'{root_path}/doc_list.txt', 'r') as file:\n",
    "        doc_list = file.read()\n",
    "        for file_name in doc_list.split('\\n'):\n",
    "            try:\n",
    "                file_path = f'{root_path}/docs/{file_name}.txt'\n",
    "                with open(file_path, 'r') as f:\n",
    "                    doc[file_name] = f.read().lower()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    print(len(query))\n",
    "    print(len(doc))\n",
    "    \n",
    "    return query, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './data/queries/.txt'\n",
      "[Errno 2] No such file or directory: './data/docs/.txt'\n",
      "100\n",
      "14955\n"
     ]
    }
   ],
   "source": [
    "query_dict, doc_dict = openFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_word_count(doc_dict):\n",
    "    word_dict = {}   # word: count\n",
    "    all_word_len = 0 # 計算 total word length in document\n",
    "\n",
    "    for doc_name, doc in doc_dict.items():\n",
    "        all_word_len += len(doc.split(' '))\n",
    "        \n",
    "        for word in doc.split(' '):\n",
    "            if word_dict.get(word, 0):\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "        \n",
    "    return word_dict, all_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111449\n",
      "7059938\n"
     ]
    }
   ],
   "source": [
    "### Calculate word count & total word length\n",
    "\n",
    "word_dict, all_word_len = cal_word_count(doc_dict)\n",
    "\n",
    "print(len(word_dict))\n",
    "print(all_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9099\n"
     ]
    }
   ],
   "source": [
    "### new word dict: 減少 word 的數量\n",
    "query_word = []\n",
    "for _, value in query_dict.items():\n",
    "    query_word.append(value.split(' '))\n",
    "query_word = sum(query_word, [])\n",
    "\n",
    "# select word if word in query or tf > 40\n",
    "new_word_dict = {}\n",
    "word2id = {}  # 對應, used in query\n",
    "id2word = {}   # 對應, used in EM algorithm\n",
    "index = 0\n",
    "\n",
    "for word in list(word_dict.keys()):\n",
    "    if word in query_word or (word_dict[word] > 50 and len(word) > 1):\n",
    "        new_word_dict[word] = word_dict[word]\n",
    "        \n",
    "        word2id[word] = index\n",
    "        id2word[index] = word\n",
    "        index += 1\n",
    "\n",
    "print(len(new_word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tf(doc_dict, new_word_dict):\n",
    "    tf_dict = {}   # tf[doc_index][word]\n",
    "    id2doc = {}   # 對應, used in write file\n",
    "    index = 0\n",
    "    \n",
    "    for doc_name, doc in doc_dict.items():\n",
    "        tf_dict[index] = {}\n",
    "        id2doc[index] = doc_name\n",
    "        \n",
    "        for word in doc.split(' '):\n",
    "            if new_word_dict.get(word, 0): # 如果在 new word dict, 才計算 tf\n",
    "                if tf_dict[index].get(word, 0):\n",
    "                    tf_dict[index][word] += 1\n",
    "                else:\n",
    "                    tf_dict[index][word] = 1\n",
    "        index += 1\n",
    "\n",
    "    return tf_dict, id2doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate tf & build mapping dict\n",
    "tf_dict, id2doc = cal_tf(doc_dict, new_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate BG word\n",
    "BG_word = {}\n",
    "for word, count in new_word_dict.items():\n",
    "    BG_word[word] = count / all_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialParameter(doc_len, word_len, K):\n",
    "    T_w = np.random.random([K, word_len])\n",
    "    d_T = np.random.random([doc_len, K])\n",
    "    \n",
    "    for k in range(0, K):\n",
    "        normalization = sum(T_w[k, :])\n",
    "        for i in range(0, word_len):\n",
    "            T_w[k, i] /= normalization\n",
    "\n",
    "    for j in range(0, doc_len):\n",
    "        normalization = sum(d_T[j, :])\n",
    "        for k in range(0, K):\n",
    "            d_T[j, k] /= normalization\n",
    "            \n",
    "    return T_w, d_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 9099)\n",
      "(14955, 32)\n",
      "(14955, 9099, 32)\n"
     ]
    }
   ],
   "source": [
    "doc_len, word_len = len(doc_dict), len(new_word_dict)\n",
    "\n",
    "# number of TOPIC\n",
    "K = 32\n",
    "\n",
    "# T_W[topic][word] : p(wi|Tk)\n",
    "# D_T[doc][topic] : p(Tk|dj)\n",
    "# e_step[doc][word][topic] : p(Tk|wi,dj)\n",
    "T_w, d_T = initialParameter(doc_len, word_len, K)\n",
    "print(T_w.shape)\n",
    "print(d_T.shape)\n",
    "\n",
    "e_step = np.zeros([doc_len,word_len,K])\n",
    "print(e_step.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EStep():\n",
    "    print('EStep: ', time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "    \n",
    "    for i in range(0, doc_len):\n",
    "        for j in range(0, word_len):\n",
    "            \n",
    "            word = id2word[j]\n",
    "            if not tf_dict[i].get(word, 0):\n",
    "                continue\n",
    "                \n",
    "            denominator = 0\n",
    "            \n",
    "            for k in range(0, K):\n",
    "                e_step[i][j][k] = T_w[k][j] * d_T[i][k]\n",
    "                denominator += e_step[i][j][k]\n",
    "                \n",
    "            if denominator == 0:\n",
    "                for k in range(0, K):\n",
    "                    e_step[i][j][k] = 0\n",
    "            else:\n",
    "                for k in range(0, K):\n",
    "                    e_step[i][j][k] /= denominator\n",
    "    \n",
    "    return e_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MStep():\n",
    "    print('MStep: ', time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "    \n",
    "    ### update T_w : p(wi|Tk)\n",
    "    for k in range(0, K):\n",
    "        denominator = 0\n",
    "        \n",
    "        for j in range(0, word_len):\n",
    "            T_w[k][j] = 0\n",
    "            word = id2word[j]\n",
    "            \n",
    "            for i in range(0, doc_len):\n",
    "                if tf_dict[i].get(word, 0):\n",
    "                    T_w[k][j] += tf_dict[i][word] * e_step[i][j][k]\n",
    "            denominator += T_w[k][j]\n",
    "            \n",
    "        if denominator == 0:\n",
    "            for j in range(0, word_len):\n",
    "                T_w[k][j] = 1.0 / word_len\n",
    "        else:\n",
    "            for j in range(0, word_len):\n",
    "                T_w[k][j] /= denominator\n",
    "                \n",
    "    ### update d_T : p(Tk|dj)\n",
    "    for i in range(0, doc_len):\n",
    "        for k in range(0, K):\n",
    "            d_T[i][k] = 0\n",
    "            denominator = 0\n",
    "            \n",
    "            for j in range(0, word_len):\n",
    "                word = id2word[j]\n",
    "                if tf_dict[i].get(word, 0):\n",
    "                    d_T[i][k] += tf_dict[i][word] * e_step[i][j][k]\n",
    "                    denominator += tf_dict[i][word]\n",
    "                \n",
    "            if denominator == 0:\n",
    "                d_T[i][k] = 1.0 / K\n",
    "            else:\n",
    "                d_T[i][k] /= denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Likelihood():\n",
    "    likelihood = 0\n",
    "    \n",
    "    for i in range(0, doc_len):\n",
    "        for j in range(0, word_len):\n",
    "            \n",
    "            word = id2word[j]\n",
    "            if not tf_dict[i].get(word, 0):\n",
    "                continue\n",
    "                \n",
    "            tmp = 0\n",
    "            for k in range(0, K):\n",
    "                tmp += T_w[k][j] * d_T[i][k]\n",
    "                \n",
    "            if tmp > 0:\n",
    "                likelihood += tf_dict[i][word] * math.log(tmp)\n",
    "                \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_algorithm():\n",
    "    Iteration = 20\n",
    "    threshold = 100.0\n",
    "    oldlikelihood = 1\n",
    "    newlikelihood = 1\n",
    "\n",
    "    for i in range(0, Iteration):\n",
    "        EStep()\n",
    "        MStep()\n",
    "        newlikelihood = Likelihood()\n",
    "        \n",
    "        print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newlikelihood))\n",
    "        \n",
    "        if ((oldlikelihood != 1) and (newlikelihood - oldlikelihood < threshold)):\n",
    "            break\n",
    "        oldlikelihood = newlikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EStep:  2020-11-25 10:20:17\n",
      "MStep:  2020-11-25 10:23:57\n"
     ]
    }
   ],
   "source": [
    "EM_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./numpy/T_w', T_w)\n",
    "np.save('./numpy/d_T',d_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLSA_model(query, doc_dict, tf_dict, BG_word, K, T_w, d_T):\n",
    "    alpha, beta = 0.8, 0.1\n",
    "    score_dict = {}\n",
    "    \n",
    "    for i in range(0, doc_len):\n",
    "        doc_name = id2doc[i]\n",
    "        doc = doc_dict[doc_name]\n",
    "        score = 1\n",
    "        \n",
    "        for word in query.split(' '):\n",
    "            tf = tf_dict[i].get(word, 0)  # 將 word 轉成 score\n",
    "            \n",
    "            tmp = 0\n",
    "            id_word = word2id[word] \n",
    "            for k in range(0, K):\n",
    "                tmp += T_w[k][id_word] * d_T[i][k]\n",
    "            \n",
    "            first = alpha * (tf / len(doc))\n",
    "            second = beta * tmp\n",
    "            third = (1 - alpha - beta) * BG_word[word]\n",
    "            \n",
    "            score *= (first + second + third)\n",
    "            \n",
    "        score_dict[doc_name] = score\n",
    "    \n",
    "    rank = sorted(score_dict.items(), key=lambda x: x[1], reverse = True) # 根據分數做排序\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('ans.txt', 'w')\n",
    "string = 'Query,RetrievedDocuments\\n'\n",
    "\n",
    "for _id, _query in query_dict.items():\n",
    "    rank = PLSA_model(_query, doc_dict, tf_dict, BG_word, K, T_w, d_T)\n",
    "\n",
    "    string += _id + ','\n",
    "    for i, doc in enumerate(rank):\n",
    "        string += doc[0] + ' '\n",
    "        \n",
    "        if i == 999:\n",
    "            break\n",
    "    string += '\\n'\n",
    "    \n",
    "f.write(string)\n",
    "f.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
