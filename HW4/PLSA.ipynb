{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFile():\n",
    "    query = {}\n",
    "    doc = {}\n",
    "    root_path = './data'\n",
    "\n",
    "    with open(f'{root_path}/query_list.txt', 'r') as file:\n",
    "        query_list = file.read()\n",
    "        for file_name in query_list.split('\\n'):\n",
    "            try:\n",
    "                file_path = f'{root_path}/queries/{file_name}.txt'\n",
    "                with open(file_path, 'r') as f:\n",
    "                    query[file_name] = f.read().lower()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    with open(f'{root_path}/doc_list.txt', 'r') as file:\n",
    "        doc_list = file.read()\n",
    "        for file_name in doc_list.split('\\n'):\n",
    "            try:\n",
    "                file_path = f'{root_path}/docs/{file_name}.txt'\n",
    "                with open(file_path, 'r') as f:\n",
    "                    doc[file_name] = f.read().lower()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    print(len(query))\n",
    "    print(len(doc))\n",
    "    \n",
    "    return query, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './data/queries/.txt'\n",
      "[Errno 2] No such file or directory: './data/docs/.txt'\n",
      "100\n",
      "14955\n"
     ]
    }
   ],
   "source": [
    "query_dict, doc_dict = openFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_word_count(doc_dict):\n",
    "    word_dict = {}   # word: count\n",
    "    all_word_len = 0 # 計算 total word length in document\n",
    "\n",
    "    for doc_name, doc in doc_dict.items():\n",
    "        all_word_len += len(doc.split(' '))\n",
    "        \n",
    "        for word in doc.split(' '):\n",
    "            if word_dict.get(word, 0):\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "        \n",
    "    return word_dict, all_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111449\n",
      "7059938\n"
     ]
    }
   ],
   "source": [
    "### Calculate word count & total word length\n",
    "\n",
    "word_dict, all_word_len = cal_word_count(doc_dict)\n",
    "\n",
    "print(len(word_dict))\n",
    "print(all_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6127\n"
     ]
    }
   ],
   "source": [
    "### new word dict: 減少 word 的數量\n",
    "query_word = []\n",
    "for _, value in query_dict.items():\n",
    "    query_word.append(value.split(' '))\n",
    "query_word = sum(query_word, [])\n",
    "\n",
    "# select word if word in query or tf > 40\n",
    "new_word_dict = {}\n",
    "for word in list(word_dict.keys()):\n",
    "    if word in query_word or word_dict[word] > 100:\n",
    "        new_word_dict[word] = word_dict[word]\n",
    "\n",
    "print(len(new_word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tf(doc_dict, new_word_dict):\n",
    "    tf_dict = {}   # tf[doc_index][word]\n",
    "    word2id =  {}  # 對應, used in EM algorithm\n",
    "    id2word = {}   # 對應, used in query\n",
    "    \n",
    "    doc_index, word_index = 0, 0\n",
    "    \n",
    "    for _, doc in doc_dict.items():\n",
    "        tf_dict[doc_index] = {}\n",
    "        \n",
    "        for word in doc.split(' '):\n",
    "            if new_word_dict.get(word, 0): # 如果在 new word dict, 才計算 tf\n",
    "                if tf_dict[doc_index].get(word, 0):\n",
    "                    tf_dict[doc_index][word] += 1\n",
    "                else:\n",
    "                    tf_dict[doc_index][word] = 1\n",
    "                    word2id[word] = word_index   # 初始化後建對應表\n",
    "                    id2word[word_index] = word\n",
    "                    word_index += 1\n",
    "        doc_index += 1\n",
    "        \n",
    "    return tf_dict, word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate tf & build mapping dict\n",
    "tf_dict, word2id, id2word = cal_tf(doc_dict, new_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'languag': 1, 'f': 4, 'p': 2, '105': 1, 'english': 1, 'articl': 1, 'type': 1, 'bfn': 1, '106': 1, 'report': 1, 'text': 1, 'march': 2, '17': 2, 'african': 1, 'nation': 2, 'congress': 1, 'anc': 4, 'presid': 2, 'nelson': 1, 'mandela': 5, 'statement': 3, 'elect': 1, 'ralli': 2, 'wednesday': 1, 'night': 1, 'parti': 2, 'mp': 1, 'dr': 4, 'said': 4, 'thursday': 1, 'mr': 4, 'told': 1, 'thousand': 1, 'support': 1, 'senior': 1, 'state': 1, 'offici': 1, 'perpetr': 1, 'violenc': 3, 'leader': 1, 'also': 2, 'charg': 1, 'hope': 1, 'april': 1, '27': 1, 'suffici': 1, 'evid': 1, 'show': 1, 'third': 2, 'forc': 2, 'intellig': 1, 'public': 1, 'commiss': 1, 'conduct': 1, 'thorough': 1, 'investig': 1, 'found': 1, 'proof': 1, 'exist': 1, 'common': 1, 'knowledg': 1, 'involv': 1, 'far': 1, 'major': 1, 'case': 1, 'intimid': 2, 'throughout': 1, 'countri': 1, 'seriou': 1, 'would': 1, 'acknowledg': 1, 'main': 1, 'someth': 1, 'hard': 1, 'hit': 1, 'undermin': 1, 'nobel': 1, 'peac': 1, 'conclud': 1}\n"
     ]
    }
   ],
   "source": [
    "for i, j in tf_dict.items():\n",
    "    print(i)\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate BG word\n",
    "BG_word = {}\n",
    "for word, count in new_word_dict.items():\n",
    "    BG_word[word] = count / all_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialParameter(doc_len, word_len, K):\n",
    "    T_w = np.random.random([K, word_len])\n",
    "    d_T = np.random.random([doc_len, K])\n",
    "    \n",
    "    for k in range(0, K):\n",
    "        normalization = sum(T_w[k, :])\n",
    "        for i in range(0, word_len):\n",
    "            T_w[k, i] /= normalization\n",
    "\n",
    "    for j in range(0, doc_len):\n",
    "        normalization = sum(d_T[j, :])\n",
    "        for k in range(0, K):\n",
    "            d_T[j, k] /= normalization\n",
    "            \n",
    "    return T_w, d_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6127)\n",
      "(14955, 2)\n",
      "(14955, 6127, 2)\n"
     ]
    }
   ],
   "source": [
    "doc_len, word_len = len(doc_dict), len(new_word_dict)\n",
    "\n",
    "# number of TOPIC\n",
    "K = 2\n",
    "\n",
    "# T_W[topic][word] : p(wi|Tk)\n",
    "# D_T[doc][topic] : p(Tk|dj)\n",
    "# e_step[doc][word][topic] : p(Tk|wi,dj)\n",
    "T_w, d_T = initialParameter(doc_len, word_len, K)\n",
    "print(T_w.shape)\n",
    "print(d_T.shape)\n",
    "\n",
    "e_step = np.zeros([doc_len,word_len,K])\n",
    "print(e_step.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EStep(doc_len, word_len, tf_dict, K, T_w, d_T, e_step):\n",
    "    print('EStep: ', time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "    \n",
    "    for i in range(0, doc_len):\n",
    "        for j in range(0, word_len):\n",
    "            \n",
    "            word = id2word[j]\n",
    "            if not tf_dict[i].get(word, 0):\n",
    "                continue\n",
    "                \n",
    "            denominator = 0\n",
    "            \n",
    "            for k in range(0, K):\n",
    "                e_step[i][j][k] = T_w[k][j] * d_T[i][k]\n",
    "                denominator += e_step[i][j][k]\n",
    "                \n",
    "            if denominator == 0:\n",
    "                for k in range(0, K):\n",
    "                    e_step[i][j][k] = 0\n",
    "            else:\n",
    "                for k in range(0, K):\n",
    "                    e_step[i][j][k] /= denominator\n",
    "    \n",
    "    return e_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MStep(doc_len, word_len, tf_dict, K, T_w, d_T, e_step):\n",
    "    print('MStep: ', time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())))\n",
    "    \n",
    "    ### update T_w : p(wi|Tk)\n",
    "    for k in range(0, K):\n",
    "        denominator = 0\n",
    "        \n",
    "        for j in range(0, word_len):\n",
    "            T_w[k][j] = 0\n",
    "            \n",
    "            for i in range(0, doc_len):\n",
    "                word = id2word[j]\n",
    "                if tf_dict[i].get(word, 0):\n",
    "                    T_w[k][j] += tf_dict[i][word] * e_step[i][j][k]\n",
    "            denominator += T_w[k][j]\n",
    "            \n",
    "        if denominator == 0:\n",
    "            for j in word_len:\n",
    "                T_w[k][j] = 1.0 / word_len\n",
    "        else:\n",
    "            for j in range(0, word_len):\n",
    "                T_w[k][j] /= denominator\n",
    "                \n",
    "    ### update d_T : p(Tk|dj)\n",
    "    for i in range(0, doc_len):\n",
    "        for k in range(0, K):\n",
    "            d_T[i][k] = 0\n",
    "            denominator = 0\n",
    "            \n",
    "            for j in range(0, word_len):\n",
    "                word = id2word[j]\n",
    "                if tf_dict[i].get(word, 0):\n",
    "                    d_T[i][k] += tf_dict[i][word] * e_step[i][j][k]\n",
    "                    denominator += tf_dict[i][word]\n",
    "                \n",
    "            if denominator == 0:\n",
    "                d_T[i][k] = 1.0 / K\n",
    "            else:\n",
    "                d_T[i][k] /= denominator\n",
    "                \n",
    "    return T_w, d_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Likelihood(doc_len, word_len, tf_dict, K, T_w, d_T):\n",
    "    likelihood = 0\n",
    "    \n",
    "    for i in range(0, doc_len):\n",
    "        for j in range(0, word_len):\n",
    "            tmp = 0\n",
    "            \n",
    "            for k in range(0, K):\n",
    "                tmp += T_w[k][j] * d_T[i][k]\n",
    "                \n",
    "            if tmp > 0:\n",
    "                word = id2word[j]\n",
    "                if tf_dict[i].get(word, 0):\n",
    "                    likelihood += tf_dict[i][word] * math.log(tmp, 10)\n",
    "                \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_algorithm(doc_len, word_len, tf_dict, K, T_w, d_T, e_step):\n",
    "    Iteration = 20\n",
    "    threshold = 100.0\n",
    "    oldLoglikelihood = 1\n",
    "    newLoglikelihood = 1\n",
    "\n",
    "    for i in range(0, Iteration):\n",
    "        e_step = EStep(doc_len, word_len, tf_dict, K, T_w, d_T, e_step)\n",
    "        T_w, d_T = MStep(doc_len, word_len, tf_dict, K, T_w, d_T, e_step)\n",
    "        newLoglikelihood = Likelihood(doc_len, word_len, tf_dict, K, T_w, d_T)\n",
    "        \n",
    "        print(\"[\", time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())), \"] \", i+1, \" iteration  \", str(newLoglikelihood))\n",
    "        \n",
    "        if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):\n",
    "            break\n",
    "        oldLoglikelihood = newLoglikelihood\n",
    "    \n",
    "    return T_w, d_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLSA_model(query, doc_dict, tf_dict, BG_word, K, T_w, d_T):\n",
    "    alpha, beta = 0.8, 0.2\n",
    "    score_dict = {}\n",
    "    \n",
    "    for i in range(0, doc_len):\n",
    "        doc_name = doc_list[i]\n",
    "        doc = doc_dict[doc_name]\n",
    "        score = 0\n",
    "        \n",
    "        for word in query.split(' '):\n",
    "            tf = tf_dict[i].get(word, 0)  # 將 word 轉成 score\n",
    "            \n",
    "            tmp = 0\n",
    "            id_word = word2id[word]\n",
    "            for k in range(0, K):\n",
    "                tmp += T_w[k][id_word] * d_T[i][k]\n",
    "            \n",
    "            first = alpha * (tf / len(doc))\n",
    "            second = beta * tmp\n",
    "            third = (1 - alpha - beta) * BG_word[word]\n",
    "            \n",
    "            score = first + second + third\n",
    "            \n",
    "        score_dict[doc_name] = score\n",
    "    \n",
    "    rank = sorted(score_dict.items(), key=lambda x: x[1], reverse = True) # 根據分數做排序\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EStep:  2020-11-24 12:39:16\n",
      "MStep:  2020-11-24 12:40:29\n"
     ]
    }
   ],
   "source": [
    "T_w, d_T = EM_algorithm(doc_len, word_len, tf_dict, K, T_w, d_T, e_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('ans.txt', 'w')\n",
    "string = 'Query,RetrievedDocuments\\n'\n",
    "\n",
    "for _id, _query in query_dict.items():\n",
    "    rank = PLSA_model(_query, doc_dict, tf_dict, BG_word, K, T_w, d_T)\n",
    "\n",
    "    string += _id + ','\n",
    "    for i, doc in enumerate(rank):\n",
    "        string += doc[0] + ' '\n",
    "        \n",
    "        if i == 999:\n",
    "            break\n",
    "    string += '\\n'\n",
    "    \n",
    "f.write(string)\n",
    "f.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
